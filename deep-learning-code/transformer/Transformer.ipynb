{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqkonOdDrg4"
      },
      "source": [
        "# Thực hành Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRy_kCLYDrhE"
      },
      "source": [
        "Trong bài này, ta sẽ thực hành cài đặt Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RePKLavXDrhF"
      },
      "source": [
        "### 1. Cài đặt và import thư viện"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WdMvH4x9swj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01941fab-5ee5-49cf-dc13-171971fd1c04"
      },
      "source": [
        "!which python3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ObXVBqjGby"
      },
      "source": [
        "!pip3 install spacy dill\n",
        "!pip3 install torchtext\n",
        "!pip3 install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US4j_5D69swl"
      },
      "source": [
        "!python3 -m spacy download en && python3 -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9vwKuktSPSx"
      },
      "source": [
        "model = spacy.load('fr')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaNedrhUjGb0"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchtext\n",
        "import copy\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6462QxLjGb0"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U7bbjULDrhN"
      },
      "source": [
        "### 2. Cài đặt từng module của Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DosoYt1YjGb0"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBaPrBG0ubI-"
      },
      "source": [
        "**Position Embedding Class**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJs0_6GwjGb1"
      },
      "source": [
        "# Positional encoding\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=300):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        \n",
        "        # create a constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, dim)\n",
        "\n",
        "        ########################\n",
        "        ##   YOUR CODE HERE   ##\n",
        "        ########################\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, dim, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/dim)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x *math.sqrt(self.dim)\n",
        "        # add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:, :seq_len], requires_grad=False).to(device)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F25tcm9tu1ce"
      },
      "source": [
        "**Multi Head Attention**: We first start with implementing attention function\n",
        "\n",
        "Attention of $q$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJEKoan4jGb2"
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vvVcNXBjGb1"
      },
      "source": [
        "# Multi-headed attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_head = dim//heads\n",
        "        self.h = heads\n",
        "        self.q_linear = nn.Linear(dim, dim)\n",
        "        self.k_linear = nn.Linear(dim, dim)\n",
        "        self.v_linear = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        bs = q.size(0)\n",
        "        # perform linear operation and split into h heads\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.dim_head)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.dim_head)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.dim_head)\n",
        "        # transpose to get dimensions bs * h * sl * dim\n",
        "        k = k.transpose(1, 2)\n",
        "        q = q.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        # calculate attention using the function we will define next\n",
        "        scores = attention(q, k, v, self.dim, mask, self.dropout)\n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.dim)\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cy0Xt9QjGb2"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7UTrblYjGb2"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uYXmKKsjGb2"
      },
      "source": [
        "# build an encoder layer with one multi-head attention layer and one \n",
        "# feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        ########################\n",
        "        ##   YOUR CODE HERE   ##\n",
        "        ########################\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AWL51G_jGb3"
      },
      "source": [
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).cuda()\n",
        "    \n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        ########################\n",
        "        ##   YOUR CODE HERE   ##\n",
        "        ########################\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \\\n",
        "        src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQAOcVwqjGb3"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "        \n",
        "    def forward(self, src, mask):\n",
        "        ########################\n",
        "        ##   YOUR CODE HERE   ##\n",
        "        ########################\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        ########################\n",
        "        ##   YOUR CODE HERE   ##\n",
        "        ########################\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVVMjazjGb3"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output# we don't perform softmax on the output as this will be handled \n",
        "# automatically by our loss function"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35cYc3ScDrhd"
      },
      "source": [
        "### 3. Chuẩn bị và tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY3841jbjGb4"
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Tokenize\n",
        "\n",
        "class tokenize(object):\n",
        "    \n",
        "    def __init__(self, lang):\n",
        "        self.nlp = spacy.load(lang)\n",
        "            \n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDB_d_mjGb4"
      },
      "source": [
        "# Creating batch\n",
        "from torchtext.legacy import data\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def nopeak_mask(size, opt):\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, opt):\n",
        "    \n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg.to(device)\n",
        "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(device)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, opt)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "        \n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask\n",
        "\n",
        "# patch on Torchtext's batching process that makes it more efficient\n",
        "# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0jx8nh0jGb5"
      },
      "source": [
        "import pandas as pd\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "import os\n",
        "import dill as pickle\n",
        "\n",
        "def read_data(opt):\n",
        "    if opt.src_data is not None:\n",
        "        try:\n",
        "            opt.src_data = open(opt.src_data).read().strip().split('\\n')\n",
        "        except:\n",
        "            print(\"error: '\" + opt.src_data + \"' file not found\")\n",
        "            quit()\n",
        "    \n",
        "    if opt.trg_data is not None:\n",
        "        try:\n",
        "            opt.trg_data = open(opt.trg_data).read().strip().split('\\n')\n",
        "        except:\n",
        "            print(\"error: '\" + opt.trg_data + \"' file not found\")\n",
        "            quit()\n",
        "\n",
        "def create_fields(opt):\n",
        "    spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n",
        "    src_lang = opt.src_lang[0:2]\n",
        "    trg_lang = opt.trg_lang[0:2]\n",
        "    if src_lang not in spacy_langs:\n",
        "        print('invalid src language: ' + opt.src_lang + 'supported languages : ' + spacy_langs)  \n",
        "    if trg_lang not in spacy_langs:\n",
        "        print('invalid trg language: ' + opt.trg_lang + 'supported languages : ' + spacy_langs)\n",
        "    \n",
        "    print(\"loading spacy tokenizers...\")\n",
        "    \n",
        "    t_src = tokenize(opt.src_lang)\n",
        "    t_trg = tokenize(trg_lang)\n",
        "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
        "\n",
        "    return(SRC, TRG)\n",
        "\n",
        "def create_dataset(opt, SRC, TRG):\n",
        "\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src' : [line for line in opt.src_data], 'trg': [line for line in opt.trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "    \n",
        "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "    \n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
        "    \n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "    SRC.build_vocab(train)\n",
        "    TRG.build_vocab(train)\n",
        "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
        "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "    opt.train_len = get_len(train_iter)\n",
        "\n",
        "    return train_iter\n",
        "\n",
        "def get_len(train):\n",
        "\n",
        "    for i, b in enumerate(train):\n",
        "        pass\n",
        "    \n",
        "    return i"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-QzvI0_Drhh"
      },
      "source": [
        "### 4. Cài đặt giải thuật tối ưu và huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXmikJB9swr"
      },
      "source": [
        "# Optimizer\n",
        "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"\n",
        "    Cosine annealing with restarts.\n",
        "    Parameters\n",
        "    ----------\n",
        "    optimizer : torch.optim.Optimizer\n",
        "    T_max : int\n",
        "        The maximum number of iterations within the first cycle.\n",
        "    eta_min : float, optional (default: 0)\n",
        "        The minimum learning rate.\n",
        "    last_epoch : int, optional (default: -1)\n",
        "        The index of the last epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 T_max: int,\n",
        "                 eta_min: float = 0.,\n",
        "                 last_epoch: int = -1,\n",
        "                 factor: float = 1.) -> None:\n",
        "        # pylint: disable=invalid-name\n",
        "        self.T_max = T_max\n",
        "        self.eta_min = eta_min\n",
        "        self.factor = factor\n",
        "        self._last_restart: int = 0\n",
        "        self._cycle_counter: int = 0\n",
        "        self._cycle_factor: float = 1.\n",
        "        self._updated_cycle_len: int = T_max\n",
        "        self._initialized: bool = False\n",
        "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"Get updated learning rate.\"\"\"\n",
        "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
        "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
        "        # last_epoch + 1 when initialized.\n",
        "        if not self._initialized:\n",
        "            self._initialized = True\n",
        "            return self.base_lrs\n",
        "\n",
        "        step = self.last_epoch + 1\n",
        "        self._cycle_counter = step - self._last_restart\n",
        "\n",
        "        lrs = [\n",
        "            (\n",
        "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
        "                (\n",
        "                    np.cos(\n",
        "                        np.pi *\n",
        "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
        "                        self._updated_cycle_len\n",
        "                    ) + 1\n",
        "                )\n",
        "            ) for lr in self.base_lrs\n",
        "        ]\n",
        "\n",
        "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
        "            # Adjust the cycle length.\n",
        "            self._cycle_factor *= self.factor\n",
        "            self._cycle_counter = 0\n",
        "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
        "            self._last_restart = step\n",
        "\n",
        "        return lrs\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRTtm5kO9swr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34a1811-c85c-4f71-8cbb-d70b5ca3124d"
      },
      "source": [
        "!mkdir data\n",
        "!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt\n",
        "!mv english.txt data\n",
        "!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt data/french.txt\n",
        "!mv french.txt data"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 16:17:46--  https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4897403 (4.7M) [text/plain]\n",
            "Saving to: ‘english.txt’\n",
            "\n",
            "english.txt         100%[===================>]   4.67M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-10-26 16:17:47 (48.5 MB/s) - ‘english.txt’ saved [4897403/4897403]\n",
            "\n",
            "--2021-10-26 16:17:47--  https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5938378 (5.7M) [text/plain]\n",
            "Saving to: ‘french.txt’\n",
            "\n",
            "french.txt          100%[===================>]   5.66M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-10-26 16:17:48 (60.5 MB/s) - ‘french.txt’ saved [5938378/5938378]\n",
            "\n",
            "--2021-10-26 16:17:48--  http://data/french.txt\n",
            "Resolving data (data)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘data’\n",
            "FINISHED --2021-10-26 16:17:48--\n",
            "Total wall clock time: 0.8s\n",
            "Downloaded: 1 files, 5.7M in 0.09s (60.5 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSOX2OEW9swr"
      },
      "source": [
        "\n",
        "def get_model(opt, src_vocab, trg_vocab):\n",
        "    \n",
        "    assert opt.d_model % opt.heads == 0\n",
        "    assert opt.dropout < 1\n",
        "\n",
        "    model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads)\n",
        "       \n",
        "    if opt.load_weights is not None:\n",
        "        print(\"loading pretrained weights...\")\n",
        "        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
        "    else:\n",
        "        for p in model.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p) \n",
        "    \n",
        "    if opt.device == 0:\n",
        "        model = model.cuda()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p4XtEsQFAKE"
      },
      "source": [
        "class Opt():\n",
        "    src_data = \"data/english.txt\"\n",
        "    trg_data = \"data/french.txt\"\n",
        "    src_lang = \"en_core_web_sm\"\n",
        "    trg_lang = \"fr_core_news_sm\"\n",
        "    epochs = 2\n",
        "    d_model=512\n",
        "    n_layers=6\n",
        "    heads=8\n",
        "    dropout=0.1\n",
        "    batchsize=1500\n",
        "    printevery=100\n",
        "    lr=0.0001\n",
        "    max_strlen=80\n",
        "    checkpoint = 0\n",
        "    no_cuda = False\n",
        "    load_weights = None\n",
        "    floyd = True\n",
        "    SGRD = True\n",
        "    optimizer = None\n",
        "    sched = None"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q2qOSP7jGb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e55fc3-1d27-430d-8f71-6c1f010e5cb4"
      },
      "source": [
        "\"\"\" BAI TAP VE NHA \"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "def train_model(model, opt):\n",
        "    ########################\n",
        "    ##   YOUR CODE HERE   ##\n",
        "    ########################\n",
        "    print(\"training model...\")\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    if opt.checkpoint > 0:\n",
        "        cptime = time.time()\n",
        "                 \n",
        "    for epoch in range(opt.epochs):\n",
        "\n",
        "        total_loss = 0\n",
        "        if opt.floyd is False:\n",
        "            print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
        "            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n",
        "        \n",
        "        if opt.checkpoint > 0:\n",
        "            torch.save(model.state_dict(), 'weights/model_weights')\n",
        "                    \n",
        "        for i, batch in enumerate(opt.train): \n",
        "\n",
        "            src = batch.src.transpose(0,1)\n",
        "            trg = batch.trg.transpose(0,1)\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "            opt.optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n",
        "            loss.backward()\n",
        "            opt.optimizer.step()\n",
        "            if opt.SGDR == True: \n",
        "                opt.sched.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if (i + 1) % opt.printevery == 0:\n",
        "                 p = int(100 * (i + 1) / opt.train_len)\n",
        "                 avg_loss = total_loss/opt.printevery\n",
        "                 if opt.floyd is False:\n",
        "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
        "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='\\r')\n",
        "                 else:\n",
        "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
        "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n",
        "                 total_loss = 0\n",
        "            \n",
        "            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
        "                torch.save(model.state_dict(), 'weights/model_weights')\n",
        "                cptime = time.time()\n",
        "   \n",
        "   \n",
        "        print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n",
        "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))\n",
        "        \n",
        "def main():\n",
        "    opt = Opt()\n",
        "    opt.src_data = \"data/english.txt\"\n",
        "    opt.trg_data = \"data/french.txt\"\n",
        "    opt.src_lang = \"en_core_web_sm\"\n",
        "    opt.trg_lang = \"fr_core_news_sm\"\n",
        "    opt.epochs = 2\n",
        "    opt.d_model=512\n",
        "    opt.n_layers=6\n",
        "    opt.heads=8\n",
        "    opt.dropout=0.1\n",
        "    opt.batchsize=1500\n",
        "    opt.printevery=100\n",
        "    opt.lr=0.0001\n",
        "    opt.max_strlen=80\n",
        "    opt.checkpoint = 0\n",
        "    opt.no_cuda = False\n",
        "    opt.load_weights = None\n",
        "    opt.floyd = True\n",
        "    opt.SGDR = True\n",
        "    \n",
        "    opt.device = 0\n",
        "    if opt.device == 0:\n",
        "        assert torch.cuda.is_available()\n",
        "    \n",
        "    read_data(opt)\n",
        "    SRC, TRG = create_fields(opt)\n",
        "    opt.train = create_dataset(opt, SRC, TRG)\n",
        "    model = get_model(opt, len(SRC.vocab), len(TRG.vocab)).to(device)\n",
        "\n",
        "    opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "    if opt.SGDR == True:\n",
        "        opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)\n",
        "    if opt.checkpoint > 0:\n",
        "        print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n",
        "    \n",
        "    train_model(model, opt)\n",
        "\n",
        "\n",
        "    # for asking about further training use while true loop, and return\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading spacy tokenizers...\n",
            "creating dataset and iterator... \n",
            "training model...\n",
            "   0m: epoch 1 [#                   ]  9%  loss = 7.043\n",
            "   1m: epoch 1 [###                 ]  18%  loss = 5.224\n",
            "   2m: epoch 1 [#####               ]  27%  loss = 4.752\n",
            "   2m: epoch 1 [#######             ]  36%  loss = 4.404\n",
            "   3m: epoch 1 [#########           ]  45%  loss = 4.180\n",
            "   4m: epoch 1 [###########         ]  55%  loss = 3.974\n",
            "   5m: epoch 1 [############        ]  64%  loss = 3.886\n",
            "   5m: epoch 1 [##############      ]  73%  loss = 3.802\n",
            "   6m: epoch 1 [################    ]  82%  loss = 3.773\n",
            "   7m: epoch 1 [##################  ]  91%  loss = 3.721\n",
            "7m: epoch 1 [####################]  100%  loss = 3.721\n",
            "epoch 1 complete, loss = 3.721\n",
            "   8m: epoch 2 [#                   ]  9%  loss = 3.685\n",
            "   9m: epoch 2 [###                 ]  18%  loss = 3.546\n",
            "   10m: epoch 2 [#####               ]  27%  loss = 3.343\n",
            "   10m: epoch 2 [#######             ]  36%  loss = 3.319\n",
            "   11m: epoch 2 [#########           ]  45%  loss = 3.245\n",
            "   12m: epoch 2 [###########         ]  55%  loss = 3.094\n",
            "   12m: epoch 2 [############        ]  64%  loss = 3.030\n",
            "   13m: epoch 2 [##############      ]  73%  loss = 3.034\n",
            "   14m: epoch 2 [################    ]  82%  loss = 2.927\n",
            "   15m: epoch 2 [##################  ]  91%  loss = 2.993\n",
            "15m: epoch 2 [####################]  100%  loss = 2.993\n",
            "epoch 2 complete, loss = 2.993\n"
          ]
        }
      ]
    }
  ]
}